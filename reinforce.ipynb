{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea03564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "episodes = 5000\n",
    "lr = 1e-4\n",
    "discount = 0.99\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "policy_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(4, 32), # 4 dimensional state vector input\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(32, 32),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(32, env.action_space.n),\n",
    "        torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "policy_net.to(device)\n",
    "\n",
    "optimzer = torch.optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac00bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/dlt3c6191j17m82qngr9v4s00000gn/T/ipykernel_29074/2012967392.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions.append(torch.tensor(action, dtype=torch.int).to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500: total reward = 75.0, steps = 75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     10\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(policy_net(obs))\n\u001b[0;32m---> 11\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     12\u001b[0m     next_obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     13\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/distributions/categorical.py:136\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    134\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[1;32m    135\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs_2d, sample_shape\u001b[38;5;241m.\u001b[39mnumel(), \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/distributions/distribution.py:261\u001b[0m, in \u001b[0;36mDistribution._extended_shape\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    Returns perplexity of distribution, batched over batch_shape.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m        Tensor of shape batch_shape.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentropy())\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extended_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample_shape: _size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize()) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize:\n\u001b[1;32m    262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    Returns the size of the sample returned by the distribution, given\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    a `sample_shape`. Note, that the batch and event shapes of a distribution\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m        sample_shape (torch.Size): the size of the sample to be drawn.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample_shape, torch\u001b[38;5;241m.\u001b[39mSize):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "    rewards, states, actions = [], [], []\n",
    "    done = False\n",
    "    totalrew = 0\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        #run action from policy\n",
    "        dist = torch.distributions.Categorical(policy_net(obs))\n",
    "        action = dist.sample()\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        #collect trajectory data\n",
    "        states.append(obs)\n",
    "        actions.append(torch.tensor(action, dtype=torch.int).to(device))\n",
    "        rewards.append(reward)\n",
    "        totalrew += reward\n",
    "        step += 1\n",
    "\n",
    "        obs = torch.tensor(next_obs, dtype=torch.float32).to(device)\n",
    "\n",
    "    if (ep + 1) % 500 == 0:\n",
    "        print(f\"Episode {ep+1}: total reward = {totalrew}, steps = {step}\")\n",
    "    returns = []\n",
    "    baseline = sum(rewards) / len(rewards)\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "        # calculate the Return/cumulative reward for each step of the trajectory\n",
    "        G = 0.0\n",
    "        for i, rew in enumerate(rewards[t:]): \n",
    "            G += (discount**i)*rew\n",
    "        returns.append(G-baseline)\n",
    "\n",
    "    for state, action, G in zip(states, actions, returns):\n",
    "        dist = torch.distributions.Categorical(policy_net(state))\n",
    "        #calculate a loss score using neagtive log liklehood\n",
    "        loss = -dist.log_prob(action) * G\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a00ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with reward 277.0\n",
      "Episode 2 finished with reward 500.0\n",
      "Episode 3 finished with reward 293.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "for episode in range(3):  # Show 3 episodes\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "        probs = policy_net(obs_tensor)\n",
    "        action = torch.argmax(probs).item()\n",
    "\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode+1} finished with reward {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

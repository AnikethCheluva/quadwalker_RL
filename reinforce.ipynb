{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea03564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "episodes = 5000\n",
    "lr = 1e-4\n",
    "discount = 0.99\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "policy_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(4, 32), # 4 dimensional state vector input\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(32, 32),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(32, env.action_space.n),\n",
    "        torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "policy_net.to(device)\n",
    "\n",
    "optimzer = torch.optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac00bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "    rewards, states, actions = [], [], []\n",
    "    done = False\n",
    "    totalrew = 0\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        dist = torch.distributions.Categorical(policy_net(obs))\n",
    "        action = dist.sample()\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        states.append(obs)\n",
    "        actions.append(torch.tensor(action, dtype=torch.int).to(device))\n",
    "        rewards.append(reward)\n",
    "        totalrew += reward\n",
    "        step += 1\n",
    "\n",
    "        obs = torch.tensor(next_obs, dtype=torch.float32).to(device)\n",
    "\n",
    "    if (ep + 1) % 500 == 0:\n",
    "        print(f\"Episode {ep+1}: total reward = {totalrew}, steps = {step}\")\n",
    "    returns = []\n",
    "    baseline = sum(rewards) / len(rewards)\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "        G = 0.0\n",
    "        for i, rew in enumerate(rewards[t:]): \n",
    "            G += (discount**i)*rew\n",
    "        returns.append(G-baseline)\n",
    "\n",
    "    for state, action, G in zip(states, actions, returns):\n",
    "        dist = torch.distributions.Categorical(policy_net(state))\n",
    "        loss = -dist.log_prob(action) * G\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a00ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "for episode in range(3):  # Show 3 episodes\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "        probs = policy_net(obs_tensor)\n",
    "        action = torch.argmax(probs).item()\n",
    "\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode+1} finished with reward {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90dc5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "episodes = 5000\n",
    "lr = 1e-3\n",
    "discount = 0.9\n",
    "batch = 10\n",
    "resets = 50\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "q_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,env.action_space.n),\n",
    ")\n",
    "\n",
    "q_net2  = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,env.action_space.n),\n",
    "\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(q_net.parameters(), lr=lr)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f154c4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/dlt3c6191j17m82qngr9v4s00000gn/T/ipykernel_23022/1626050119.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = q_net(torch.tensor(s[0], dtype=torch.float32).to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500: steps = 10\n",
      "Episode 1000: steps = 17\n",
      "Episode 1500: steps = 72\n",
      "Episode 2000: steps = 138\n",
      "Episode 2500: steps = 126\n",
      "Episode 3000: steps = 165\n",
      "Episode 3500: steps = 119\n",
      "Episode 4000: steps = 218\n",
      "Episode 4500: steps = 213\n",
      "Episode 5000: steps = 259\n"
     ]
    }
   ],
   "source": [
    "replay = []\n",
    "totalsteps = 0\n",
    "\n",
    "\n",
    "for ep in range(episodes):\n",
    "    steps = 0\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "    done = False\n",
    "    while not done:\n",
    "        #sample action based on greedy or Q values\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randint(0, 1)\n",
    "        else:\n",
    "            action = torch.argmax(q_net(obs)).item()\n",
    "\n",
    "        #execute action\n",
    "        newobs, reward, terminated, truncated, _ = env.step(action)\n",
    "        steps += 1\n",
    "        totalsteps += 1\n",
    "        done = terminated or truncated\n",
    "\n",
    "        #store transition\n",
    "        replay.append([obs, action, reward, newobs, done])\n",
    "        \n",
    "        #reset current state\n",
    "        obs = torch.tensor(newobs, dtype=torch.float32).to(device)\n",
    "    \n",
    "    if (ep + 1) % 500 == 0:\n",
    "        print(f\"Episode {ep+1}: steps = {steps}\")\n",
    "\n",
    "    #check if enough transitions to sample a whole batch\n",
    "    if len(replay) > batch:\n",
    "        sample =  random.sample(replay, batch)\n",
    "    else:\n",
    "        sample = replay\n",
    "\n",
    "    #sample batch\n",
    "    for s in sample:\n",
    "        if s[4]:\n",
    "            #if transiiton is the last in episode set target to the reward\n",
    "            target = s[2]\n",
    "        else:\n",
    "            #if transition is not the last episode\n",
    "            #collect future q_vals from state transitioned to\n",
    "            future_qvals = q_net2(torch.tensor(s[3], dtype=torch.float32).to(device))\n",
    "            #target is the immideate reward + best reward possible from future\n",
    "            target = s[2] + discount * torch.max(future_qvals).item()\n",
    "        \n",
    "        actions = q_net(torch.tensor(s[0], dtype=torch.float32).to(device))\n",
    "        #collect qval for the action taken in obs\n",
    "        qval = actions[s[1]]\n",
    "\n",
    "        #find mean squared error between the target qval found from looking at future rewards\n",
    "        loss = (target - qval)**2\n",
    "\n",
    "        #update params based on loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    #every so often update the behavior policy to the target policy\n",
    "    if totalsteps % 100 == 0:\n",
    "        q_net2.load_state_dict(q_net.state_dict())\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "648f4126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with reward 500.0\n",
      "Episode 2 finished with reward 311.0\n",
      "Episode 3 finished with reward 500.0\n",
      "Episode 4 finished with reward 500.0\n",
      "Episode 5 finished with reward 500.0\n",
      "Episode 6 finished with reward 284.0\n",
      "Episode 7 finished with reward 330.0\n",
      "Episode 8 finished with reward 317.0\n",
      "Episode 9 finished with reward 500.0\n",
      "Episode 10 finished with reward 500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "for episode in range(10):  # Show 3 episodes\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "        probs = q_net(obs_tensor)\n",
    "        action = torch.argmax(probs).item()\n",
    "\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode+1} finished with reward {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
